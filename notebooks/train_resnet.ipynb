{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7e44c3ed-dfe5-4693-a145-8fa12f451c2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: http://mirrors.aliyun.com/pypi/simple\n",
      "Requirement already satisfied: scikit-learn in ./miniconda3/lib/python3.12/site-packages (1.6.1)\n",
      "Requirement already satisfied: matplotlib in ./miniconda3/lib/python3.12/site-packages (3.9.2)\n",
      "Requirement already satisfied: numpy>=1.19.5 in ./miniconda3/lib/python3.12/site-packages (from scikit-learn) (2.1.3)\n",
      "Requirement already satisfied: scipy>=1.6.0 in ./miniconda3/lib/python3.12/site-packages (from scikit-learn) (1.15.2)\n",
      "Requirement already satisfied: joblib>=1.2.0 in ./miniconda3/lib/python3.12/site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in ./miniconda3/lib/python3.12/site-packages (from scikit-learn) (3.6.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in ./miniconda3/lib/python3.12/site-packages (from matplotlib) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in ./miniconda3/lib/python3.12/site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in ./miniconda3/lib/python3.12/site-packages (from matplotlib) (4.55.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in ./miniconda3/lib/python3.12/site-packages (from matplotlib) (1.4.7)\n",
      "Requirement already satisfied: packaging>=20.0 in ./miniconda3/lib/python3.12/site-packages (from matplotlib) (23.2)\n",
      "Requirement already satisfied: pillow>=8 in ./miniconda3/lib/python3.12/site-packages (from matplotlib) (11.0.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in ./miniconda3/lib/python3.12/site-packages (from matplotlib) (3.2.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in ./miniconda3/lib/python3.12/site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: six>=1.5 in ./miniconda3/lib/python3.12/site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install scikit-learn matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "099ffce2-4505-4de5-b15e-b72a6ec6a642",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/10, Loss: 231.3830, Accuracy: 76.60%\n",
      "Epoch 2/10, Loss: 136.9895, Accuracy: 86.18%\n",
      "Epoch 3/10, Loss: 96.6853, Accuracy: 90.31%\n",
      "Epoch 4/10, Loss: 75.7803, Accuracy: 91.94%\n",
      "Epoch 5/10, Loss: 68.6252, Accuracy: 92.99%\n",
      "Epoch 6/10, Loss: 70.3728, Accuracy: 93.14%\n",
      "Epoch 7/10, Loss: 40.5753, Accuracy: 95.75%\n",
      "Epoch 8/10, Loss: 38.4033, Accuracy: 96.17%\n",
      "Epoch 9/10, Loss: 51.2616, Accuracy: 94.53%\n",
      "Epoch 10/10, Loss: 29.4431, Accuracy: 96.95%\n",
      "Model saved to ./model_output/resnet18.pth\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      " Agriculture       0.97      0.95      0.96       156\n",
      "     Airport       0.94      0.66      0.77       157\n",
      "       Beach       0.83      0.99      0.90       156\n",
      "        City       0.89      0.88      0.89       155\n",
      "      Desert       0.92      0.88      0.90       162\n",
      "      Forest       0.95      0.94      0.95       178\n",
      "   Grassland       0.92      0.93      0.92       150\n",
      "     Highway       0.88      0.89      0.89       147\n",
      "        Lake       0.86      0.96      0.91       165\n",
      "    Mountain       0.86      0.82      0.84       173\n",
      "     Parking       0.96      0.96      0.96       172\n",
      "        Port       0.97      0.88      0.92       167\n",
      "     Railway       0.90      0.86      0.88       165\n",
      " Residential       0.79      0.98      0.87       169\n",
      "       River       0.85      0.86      0.86       129\n",
      "\n",
      "    accuracy                           0.90      2401\n",
      "   macro avg       0.90      0.90      0.89      2401\n",
      "weighted avg       0.90      0.90      0.90      2401\n",
      "\n",
      "Confusion Matrix:\n",
      "[[148   0   1   0   2   0   1   1   1   0   0   0   0   1   1]\n",
      " [  1 103   3   2   0   0   0   6   2   6   1   3   9  13   8]\n",
      " [  0   0 154   0   0   1   0   0   0   0   0   0   0   0   1]\n",
      " [  0   0   1 136   0   0   0   0   1   1   2   0   2  12   0]\n",
      " [  0   0  10   0 143   0   1   0   1   7   0   0   0   0   0]\n",
      " [  0   0   0   0   0 167   6   0   0   0   0   0   0   4   1]\n",
      " [  0   0   2   0   0   1 139   1   4   0   0   0   0   0   3]\n",
      " [  0   4   1   0   0   0   0 131   1   0   0   0   3   4   3]\n",
      " [  0   0   2   0   0   0   1   1 159   1   0   0   1   0   0]\n",
      " [  0   0   0   5  10   5   2   0   2 142   0   0   0   7   0]\n",
      " [  0   0   1   0   0   0   0   3   0   0 165   2   0   1   0]\n",
      " [  0   0   6   3   0   0   0   1   7   0   1 147   0   0   2]\n",
      " [  1   2   2   6   0   0   0   5   3   0   1   0 142   3   0]\n",
      " [  0   0   2   0   0   0   0   0   0   0   2   0   0 165   0]\n",
      " [  2   1   0   0   0   1   1   0   3   9   0   0   1   0 111]]\n",
      "Total time: 186.96s\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision import datasets, models\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import shutil\n",
    "from torchvision.models import ResNet18_Weights\n",
    "\n",
    "DATA_DIR = '/root/Aerial_Landscapes'\n",
    "BATCH_SIZE = 32\n",
    "NUM_CLASSES = 15\n",
    "NUM_EPOCHS = 10\n",
    "LEARNING_RATE = 1e-3\n",
    "MODEL_SAVE_PATH = './model_output/resnet18.pth'\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "illegal_dir = os.path.join(DATA_DIR, '.ipynb_checkpoints')\n",
    "if os.path.exists(illegal_dir):\n",
    "    shutil.rmtree(illegal_dir)\n",
    "else:\n",
    "    _ = print(\"\")\n",
    "\n",
    "\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(15),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "from torchvision.datasets import ImageFolder\n",
    "\n",
    "def is_valid_image(filename):\n",
    "    return filename.lower().endswith(('.jpg', '.jpeg', '.png'))\n",
    "\n",
    "full_dataset = ImageFolder(root=DATA_DIR, transform=transform_train, is_valid_file=is_valid_image)\n",
    "class_names = full_dataset.classes\n",
    "\n",
    "train_size = int(0.8 * len(full_dataset))\n",
    "val_size = len(full_dataset) - train_size\n",
    "train_dataset, val_dataset = random_split(full_dataset, [train_size, val_size])\n",
    "val_dataset.dataset.transform = transform_test  # Use test transform in validation\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "model = models.resnet18(weights=ResNet18_Weights.DEFAULT)\n",
    "model.fc = nn.Linear(model.fc.in_features, NUM_CLASSES)\n",
    "model = model.to(DEVICE)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "def train():\n",
    "    model.train()\n",
    "    os.makedirs(os.path.dirname(MODEL_SAVE_PATH), exist_ok=True)\n",
    "    \n",
    "    acc_list = []\n",
    "    loss_list = []\n",
    "\n",
    "    for epoch in range(NUM_EPOCHS):\n",
    "        running_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "\n",
    "        for images, labels in train_loader:\n",
    "            images, labels = images.to(DEVICE), labels.to(DEVICE)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += labels.size(0)\n",
    "            correct += predicted.eq(labels).sum().item()\n",
    "\n",
    "        acc = 100. * correct / total\n",
    "        acc_list.append(acc)\n",
    "        loss_list.append(running_loss)\n",
    "        print(f\"Epoch {epoch+1}/{NUM_EPOCHS}, Loss: {running_loss:.4f}, Accuracy: {acc:.2f}%\")\n",
    "\n",
    "    np.save(\"acc_resnet18.npy\", np.array(acc_list))\n",
    "    np.save(\"loss_resnet18.npy\", np.array(loss_list))\n",
    "    torch.save(model.state_dict(), MODEL_SAVE_PATH)\n",
    "    print(f\"Model saved to {MODEL_SAVE_PATH}\")\n",
    "\n",
    "def evaluate():\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in val_loader:\n",
    "            images = images.to(DEVICE)\n",
    "            outputs = model(images)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.numpy())\n",
    "\n",
    "    print(\"Classification Report:\")\n",
    "    print(classification_report(all_labels, all_preds, target_names=class_names))\n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(confusion_matrix(all_labels, all_preds))\n",
    "\n",
    "start = time.time()\n",
    "train()\n",
    "evaluate()\n",
    "print(f\"Total time: {(time.time() - start):.2f}s\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cc52df4-e490-4756-8492-d1303467ddd5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22230080-e064-4301-b34b-5662a87b4131",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "439e9d0c-d0a1-49fa-ab5d-fb96dc1b3b18",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
